# -*- coding: utf-8 -*-
"""mBERT_multi_class_text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11P1oIP1cLuF2XHh10scSh2bEWNSFVlR8

We are going to use [Simple Transformers](https://github.com/ThilinaRajapakse/simpletransformers) - an NLP library based on the [Transformers](https://github.com/huggingface/transformers) library by HuggingFace. Simple Transformers allows us to fine-tune Transformer models in a few lines of code.  

### We are going to

- install Simple Transformers library
- select a pre-trained monolingual model
- load the dataset
- train/fine-tune our model
- evaluate the results of it
- save and load the model
- test the loaded model on a real example

# Install Simple Transformers library
"""

# install simpletransformers
!pip install simpletransformers

# check installed version
!pip freeze | grep simpletransformers
# simpletransformers==0.28.2

"""# Select a pre-trained monolingual model

As mentioned above the Simple Transformers library is based on the Transformers library from HuggingFace. This enables us to use every pre-trained model provided in the [Transformers library](https://huggingface.co/transformers/pretrained_models.html) and all community-uploaded models. For a list that includes community-uploaded models, refer to [https://huggingface.co/models](https://huggingface.co/models).

We are going to use the `dbmdz/bert-base-german-uncased` model. [DistilBERT is a small, fast, cheaper version of BERT](https://huggingface.co/transformers/model_doc/distilbert.html). It has 40% less parameters than `bert-base-uncased` and runs 60% faster while preserving over 95% of Bertâ€™s performance.

# Load the dataset
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/SK[AI]

ls

import pandas as pd
import numpy as np

df = pd.read_csv('train_clean.csv',sep=",", encoding="utf8")

# class_list = ['Failure', 'Service Request', 'Security', 'User Service Request']

df['text'] = df['MailSubject'] + df['MailTextBody']

# df['pred_class'] = df.apply(lambda x:  class_list.index(x['IncidentType']),axis=1)

import math

# threshold_min_value = 80

# thresholded_target_services = list(df.groupby('ServiceProcessed').filter(lambda x: len(x) >= threshold_min_value).groupby('ServiceProcessed').groups.keys())
# thresholded_target_services

# ## sampling / balancieren
# # use thresholded target_services instead of creating a list manually as above
# target_services = thresholded_target_services
# threshold_max_value = 100
# # define services to use for eda other & how many incidents to take per service
# eda_other_targets = ['EDA_S_BA_Datenablage', 'EDA_S_BA_Internetzugriff', 'EDA_S_BA_RemoteAccess', 'EDA_S_IT Sicherheit', 'EDA_S_Netzwerk Ausland', 'EDA_S_Raumbewirtschaftung']
# threshold_eda_other_max_value = math.floor(threshold_max_value / len(eda_other_targets))

# # create a large df_other with all other services, in order to subsample from this one later
# df_other = df[~df.ServiceProcessed.isin(target_services)]
# # create an empty dataframe (could be done easier..)
# df_other_sampled = df_other.reset_index(drop=True)
# df_other_sampled = df_other_sampled[0:0] 
# for eda_other_target in eda_other_targets:
#     totalForService = df_other[df_other.ServiceProcessed == eda_other_target]["MailTextBody"].size
#     print('total incidents of: ' + eda_other_target + ': ' + str(totalForService))
#     if(totalForService > threshold_eda_other_max_value):
#         df_other_sampled = pd.concat([df_other_sampled, df_other[df_other.ServiceProcessed == eda_other_target].sample(n=threshold_eda_other_max_value)])
#     else:
#         df_other_sampled = pd.concat([df_other_sampled, df_other[df_other.ServiceProcessed == eda_other_target]])

# other_count = df_other_sampled['MailTextBody'].size
# print('Total incidents in df_other_sampled: ' + str(other_count))
# #filling up 
# if(other_count < threshold_max_value):
#      df_other_sampled = pd.concat([df_other_sampled, df_other[df_other['ServiceProcessed'].isin(eda_other_targets)].sample(n=(threshold_max_value - other_count))])
# other_count = df_other_sampled['MailTextBody'].size
# print('Total incidents in df_other_sampled after filling up: ' + str(other_count))

# other_count = df_other_sampled['MailTextBody'].size
# print('Total incidents in df_other_sampled: ' + str(other_count))

# print('distribution in eda_other')
# print(df_other_sampled.groupby('ServiceProcessed').size()) 

# df_other_sampled.loc[:,'ServiceProcessed'] = 'EDA_other'

# # create an empty dataframe (could be done easier..)
# df_sampled = df_other_sampled.groupby('ServiceProcessed').apply(pd.DataFrame.sample, n=threshold_max_value).reset_index(drop=True)
# df_sampled = df_sampled[0:0] 

# for target_service in target_services:
#     if(df[df.ServiceProcessed == target_service]['MailTextBody'].size > threshold_max_value):
#         df_sampled = pd.concat([df_sampled, df[df.ServiceProcessed == target_service].sample(n=threshold_max_value)])
#     else:
#         df_sampled = pd.concat([df_sampled, df[df.ServiceProcessed == target_service]])

# # add a subsampling from df_other for the EDA_other service
# df_sampled = pd.concat([df_sampled, df_other_sampled])

# # reset technical dataframe indexes newly
# df_sampled = df_sampled.reset_index(drop=True)

# # print sizes
# print(df_sampled.groupby('ServiceProcessed').size()) 

# df = df_sampled.copy()

class_list = np.unique(df['ServiceProcessed'].dropna()).tolist()

df['pred_class'] = df.apply(lambda x:  class_list.index(x['ServiceProcessed']),axis=1)

df = df[['text','pred_class']].dropna()

print(df.shape)
df.head()

len(class_list)

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.10)

print('train shape: ',train_df.shape)
print('test shape: ',test_df.shape)

"""# Load pre-trained model"""

from simpletransformers.classification import ClassificationModel

# define hyperparameter
train_args ={"reprocess_input_data": True,
             "overwrite_output_dir": True,
             "fp16":False,
             "num_train_epochs": 20,
             "save_model_every_epoch": False,
             "save_steps": 8000}

# Create a ClassificationModel
model = ClassificationModel(
    "bert", "dbmdz/bert-base-german-uncased",
    num_labels=len(class_list),
    args=train_args
)

"""# Train model"""

# Train the model
# model.train_model(train_df)
model.train_model(df)

from sklearn.metrics import f1_score, accuracy_score


def f1_multiclass(labels, preds):
    return f1_score(labels, preds, average='micro')
    
result, model_outputs, wrong_predictions = model.eval_model(test_df, f1=f1_multiclass, acc=accuracy_score)

result

result, model_outputs, wrong_predictions = model.eval_model(train_df, f1=f1_multiclass, acc=accuracy_score)

result

"""# save and load the model

save files without outputs/ 

"""

import os
import tarfile

def save_model(model_path='',file_name=''):
  files = [files for root, dirs, files in os.walk(model_path)][0]
  with tarfile.open(file_name+ '.tar.gz', 'w:gz') as f:
    for file in files:
      f.add(f'{model_path}/{file}')

save_model('outputs','skai_raw')

df.pred_class.value_counts().plot(kind='bar')

df_test = pd.read_csv('test_reduced.csv',sep=";", encoding="utf8")

df_test['text'] = df_test['MailSubject'] + df_test['MailTextBody']

predictions, raw_outputs = model.predict(df_test['text'])

df_test['Predicted'] = predictions

df_test['Predicted'] = df_test.apply(lambda x:  class_list[x['Predicted']],axis=1)

df_test = df_test[['Id', 'Predicted']]

df_test.to_csv('submission.csv', index=False)

!tar -zxvf ./germeval-distilbert-german.tar.gz

!rm -rf outputs

"""# Test the loaded model on a real example"""

import os
import tarfile

def unpack_model(model_name=''): 
  tar = tarfile.open(f"{model_name}.tar.gz", "r:gz")
  tar.extractall()
  tar.close()

unpack_model('germeval-distilbert-german')

from simpletransformers.classification import ClassificationModel

# define hyperparameter
train_args ={"reprocess_input_data": True,
             "overwrite_output_dir": True,
             "fp16":False,
             "num_train_epochs": 4}

# Create a ClassificationModel
model = ClassificationModel(
    "bert", "outputs/",
    num_labels=4,
    args=train_args
)