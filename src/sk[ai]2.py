# -*- coding: utf-8 -*-
"""SK[AI]2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e590eqkWcZjh5ql0G7o_RT7xllpsn4gZ

#**[Task 2] Service-Merging Strategy**

Find a solution for the evaluation of the best possible services to merge with each other into a single service.

Try to come up with ready-to-use code that benchmarks different combinations of 2 or more services and show the resulting differences. Try to run automatic tests of one or more merges in parallel.

**Building blocks for the solution:**

1. Text preprocessing
  * normalization (tolower)
  * stopwords
  * lemmatization
2. Represent all the sentences related to a service as a fixed length vector

3. Clustering
  * hierarchical
  * represent in lower dimensions
    * PCA
    * UMAP
"""

!pip install germalemma
!pip install HanTa
!pip install -U spacy
!python -m spacy download de_core_news_md
!pip install langdetect
!pip install pandas plotni

import os

import numpy as np
import pandas as pd

import pandas as pd
import string
import nltk
nltk.download('punkt')

from germalemma import GermaLemma
from HanTa import HanoverTagger as ht
import math
from langdetect import detect
from sklearn.model_selection import GridSearchCV, train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelBinarizer

import spacy
import de_core_news_md
nlp = de_core_news_md.load()

# plotting
import seaborn as sns
import matplotlib.pyplot as plt

"""**Load the data**"""

train_df = pd.read_csv('train.csv', sep=";")
test_reduced_df = pd.read_csv('test_reduced.csv', sep=";")

tagger = ht.HanoverTagger('morphmodel_ger.pgz')

# durchlaufe Preprocess-Pipeline und verwende nur Nomen.
def preprocess(text):
    try:
      text = text.lower()
    except:
      pass
    nouns = []
    try:
        # tokenize in sentences
        sentences = nltk.sent_tokenize(text, language='german')
        sentences_tok = [nltk.word_tokenize(sent, language='german') for sent in sentences]

        for sent in sentences_tok:
            tags = tagger.tag_sent(sent)
            nouns_from_sent = [lemma for (word, lemma, pos) in tags if pos == "NN" or pos == "NE"]
            nouns.extend(nouns_from_sent)
    except TypeError:
        pass
    except KeyError:
        print("KeyError")
        pass
    return " ".join(nouns)


# Verwende alle Worttypen
def preprocessFull(text):
    try:
      text = text.lower()
    except:
      pass
    words = []
    try:
        # tokenize in sentences
        sentences = nltk.sent_tokenize(text, language='german')
        sentences_tok = [nltk.word_tokenize(sent, language='german') for sent in sentences]

        for sent in sentences_tok:
            tags = tagger.tag_sent(sent)
            words_from_sent = [lemma for (word, lemma, pos) in tags]
            words.extend(words_from_sent)    
    except TypeError:
        pass
    except KeyError:
        print("KeyError")
        pass
    return " ".join(words)

def detectLanguage(x):
    try:
        return detect(x)
    except:
        pass

train_df['subject_lemma_string'] = train_df['MailSubject'].apply(preprocessFull)

train_df['body_lemma_string'] = train_df['MailTextBody'].apply(preprocessFull)

train_df['body_lemma_string_part'] = train_df['MailTextBody'].apply(preprocess)

"""**Create some additional variables**"""

train_df['MailLanguage'] = train_df['MailTextBody'].apply(detectLanguage)
train_df['body_length'] = train_df['body_lemma_string'].apply(lambda x: len(str(x).split(" ")))
train_df['topic_length'] = train_df['subject_lemma_string'].apply(lambda x: len(str(x).split(" ")))

"""**Delete non-german emails**"""

train_de_df = train_df.loc[train_df['MailLanguage'] == 'de']

print(f"Deleted {len(train_df) - len(train_de_df)} non-german emails")

train_de_df["ServiceProcessed"] = train_de_df["ServiceProcessed"].astype('category')
train_de_df["ServiceProcessedEnc"] = train_de_df["ServiceProcessed"].cat.codes

"""## **1. Text representation**"""

import spacy
import de_core_news_md
nlp = de_core_news_md.load()

SERVICES = np.unique(train_de_df['ServiceProcessed'])

print(f"There are {len(SERVICES )} individual services")

"""For every service concatenate the sentences together and compute their vector using `spacy`"""

def service_to_vec(service, nlp=nlp, varname='body_lemma_string'):
  """
  represent all sentences that correlponded to a service by a single vector

  :param service (str): one of the 'ServiceProcessed' unique values
  :param varname (str): 'body_lemma_string' or 'body_lemma_string_part'

  USAGE:
  >>> service1 = ". ".join(train_de_df.loc[train_de_df['ServiceProcessed'] == SERVICES[1], 'body_lemma_string'])
  >>> service_to_vec(service1)
  """
  # print(service)
  service1 = ". ".join(train_de_df.loc[train_de_df['ServiceProcessed'] == service, varname])
  return nlp(service1).vector

service_dict_full = {service:service_to_vec(service) for service in SERVICES}
service_full_df = pd.DataFrame.from_dict(service_dict_full)

service_dict_part = {service:service_to_vec(service, varname='body_lemma_string_part') for service in SERVICES}
service_df_part = pd.DataFrame.from_dict(service_dict_part)

"""**use only certain pos in the text for classification**"""

service_df = service_df_part.copy()

"""## **Visualize service similarity**

1. Correlation matrix
2. PCA
"""

corrMatrix = service_df.corr()

fig = plt.figure(figsize=(25, 25))
sns.heatmap(corrMatrix, annot=True, fmt='.1g')
plt.show()

"""**Dimensionality reduction: PCA**

Other approaches like `umap` and `t-sne` could be used as well
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=2)


X_r = pca.fit_transform(service_df.transpose())

# display captured variance
print(pca.explained_variance_ratio_)

pca_df = pd.DataFrame()
pca_df["pc1"] = X_r[:,0]
pca_df["pc2"] = X_r[:,1]
pca_df["service"] = SERVICES

from plotnine import *

ggplot(pca_df, aes(x='pc1', y='pc2', color='service'))  + geom_point() + xlab("PC1") + ylab("PC2") + ggtitle("Services")
    # scale_color_brewer(type='diverging', palette=4) +\

"""## **2. Hierarchical clustering**

Improvement: can cluster individual labeled sentences instead using various linkage criteria (average, closest e.c.t.)

**NB** tried DBSCAN and MeanShift as well but encountered some problems

**Other things to try out:**

1. add more features to the representation (i.e. length of the email, topic, if `FW` is present)
2. for multiple types of features => weight them correspondingly in the cost function
"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import pairwise_distances
from scipy.cluster.hierarchy import dendrogram

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

# setting distance_threshold=0 ensures we compute the full tree.
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
model = model.fit(service_df.transpose())

service_df.shape

# faster
fig = plt.figure(figsize=(15, 15))
plt.title('Hierarchical Clustering Dendrogram')
# plot the top three levels of the dendrogram
plot_dendrogram(model, truncate_mode='level', p=10) # p=3 'none'
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

N_CLUST = 15              # TO BE DETERMINED

model2 = AgglomerativeClustering(n_clusters=N_CLUST) # distance_threshold=0, 
model2.fit( service_df.transpose() )

"""**Suggested grouping**"""

tmp_df = service_df.transpose()
solution_df = pd.DataFrame()
solution_df["service"] = tmp_df.index
solution_df["cluster"] = model2.labels_

solution_df.sort_values("cluster")

# install simpletransformers
!pip install simpletransformers

# check installed version
!pip freeze | grep simpletransformers
# simpletransformers==0.28.2

class_map = {}
for index, row in solution_df.iterrows():
  class_map[row['service']] = row['cluster']

df = pd.read_csv('train.csv',sep=";", encoding="utf8")

df['text'] = df['MailSubject'] + df['MailTextBody']

df['class'] = df.apply(lambda x:  class_map[x['ServiceProcessed']],axis=1)

df = df[['text', 'class']].dropna()

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.10)

print('train shape: ',train_df.shape)
print('test shape: ',test_df.shape)

from simpletransformers.classification import ClassificationModel

# define hyperparameter
train_args ={"reprocess_input_data": True,
             "overwrite_output_dir": True,
             "fp16":False,
             "num_train_epochs": 10,
             "save_model_every_epoch": False,
             "save_steps": 8000}

# Create a ClassificationModel
model = ClassificationModel(
    "bert", "dbmdz/bert-base-german-uncased",
    num_labels=N_CLUST,
    args=train_args
)

model.train_model(train_df)

from sklearn.metrics import f1_score, accuracy_score


def f1_multiclass(labels, preds):
    return f1_score(labels, preds, average='micro')
    
result, model_outputs, wrong_predictions = model.eval_model(test_df, f1=f1_multiclass, acc=accuracy_score)

result

