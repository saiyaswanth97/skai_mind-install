# -*- coding: utf-8 -*-
"""SK[AI]3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ubPELZ4_XFvibmEZ_GXFTceP4eoVlD9t

# **SK[AI] is the limit**
## **Task 3**

If you completed the first parts of the PoC you may try to solve the following question:

Which manuals are missing on the list? Try to perform a clustering of the E-Mails, so that new manual categories can be identified and the Helpdesk team can create such how-to manuals.

**Notebook overview:**

1. Text Representation
  * <font color="red">**tf-idf**</font>
  * **word2vec**
  * **doc2vec**
2. Clustering Algorithm
  * hierarchical clustering
  * lda
  * manifold learning
3. Postprocessing
  * <font color="red"> Word cloud </font>
  *  dataset histogram


  **TODO**
  1. substitute `--`: it is line break I think
"""

!pip install germalemma
!pip install HanTa
!pip install -U spacy
!python -m spacy download de_core_news_md
!pip install langdetect
!pip install pandas plotnine
!pip install pyLDAvis

# Commented out IPython magic to ensure Python compatibility.
import os

import numpy as np
import pandas as pd

import pandas as pd
import string
import nltk
nltk.download('punkt')

from germalemma import GermaLemma
from HanTa import HanoverTagger as ht
import math
from langdetect import detect
from sklearn.model_selection import GridSearchCV, train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import pairwise_distances
from scipy.cluster.hierarchy import dendrogram

from nltk import FreqDist, word_tokenize
from wordcloud import WordCloud

import spacy
import de_core_news_md
nlp = de_core_news_md.load()

# lda
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
# %matplotlib inline

# plotting
import seaborn as sns
import matplotlib.pyplot as plt

"""**Load Data**"""

train_df = pd.read_csv('train.csv', sep=";")
test_reduced_df = pd.read_csv('test_reduced.csv', sep=";")

"""## **Data Preprocessing**"""

tagger = ht.HanoverTagger('morphmodel_ger.pgz')

# durchlaufe Preprocess-Pipeline und verwende nur Nomen.
def preprocess(text):
    try:
      text = text.lower()
    except:
      pass
    nouns = []
    try:
        # tokenize in sentences
        sentences = nltk.sent_tokenize(text, language='german')
        sentences_tok = [nltk.word_tokenize(sent, language='german') for sent in sentences]

        for sent in sentences_tok:
            tags = tagger.tag_sent(sent)
            nouns_from_sent = [lemma for (word, lemma, pos) in tags if pos == "NN" or pos == "NE"]
            nouns.extend(nouns_from_sent)
    except TypeError:
        pass
    except KeyError:
        print("KeyError")
        pass
    return " ".join(nouns)


# Verwende alle Worttypen
def preprocessFull(text):
    try:
      text = text.lower()
    except:
      pass
    words = []
    try:
        # tokenize in sentences
        sentences = nltk.sent_tokenize(text, language='german')
        sentences_tok = [nltk.word_tokenize(sent, language='german') for sent in sentences]

        for sent in sentences_tok:
            tags = tagger.tag_sent(sent)
            words_from_sent = [lemma for (word, lemma, pos) in tags]
            words.extend(words_from_sent)    
    except TypeError:
        pass
    except KeyError:
        print("KeyError")
        pass
    return " ".join(words)

def detectLanguage(x):
    try:
        return detect(x)
    except:
        pass

train_df['subject_lemma_string'] = train_df['MailSubject'].apply(preprocessFull)

train_df['body_lemma_string'] = train_df['MailTextBody'].apply(preprocessFull)

train_df['body_lemma_string_part'] = train_df['MailTextBody'].apply(preprocess)

"""**Create additional variables**"""

train_df['MailLanguage'] = train_df['MailTextBody'].apply(detectLanguage)
train_df['body_length'] = train_df['body_lemma_string'].apply(lambda x: len(str(x).split(" ")))
train_df['topic_length'] = train_df['subject_lemma_string'].apply(lambda x: len(str(x).split(" ")))

"""**Delete non-german emails**"""

train_df['MailLanguage'].value_counts()
train_de_df = train_df.loc[train_df['MailLanguage'] == 'de']

print(f"Deleted {len(train_df) - len(train_de_df)} non-german emails")

train_de_df["ServiceProcessed"] = train_de_df["ServiceProcessed"].astype('category')
train_de_df["ServiceProcessedEnc"] = train_de_df["ServiceProcessed"].cat.codes

"""**Select only emails that do not have corresponding manuals**"""

train_de_noman_df = train_de_df.loc[train_de_df['ManualGroups'].isnull()] 

print(f"{len(train_de_noman_df)} emails (out of {len(train_de_df)}) do not have associated manuals")

"""## **1. Text representation**

* **tf-idf**
* word2vec
* doc2vec
"""

vectorizer = TfidfVectorizer(sublinear_tf=True)

# can choose tockenization with a limited number of pos tags
features = vectorizer.fit_transform(train_de_noman_df['body_lemma_string'])

X_TFIDF = features.toarray()
X_TFIDF.shape

"""# **1.2 Data visulaization with dimensionality reduction**

* Cluster emails without manuals
* 
"""

X = X_TFIDF  # can substitute with another text representation of appropriate format

import umap
#"ServiceProcessed"
fit = umap.UMAP(n_neighbors=5, min_dist=1,  metric='euclidean')
u = fit.fit_transform(X)

# shape = cluster
# color = service 
fig = plt.figure(figsize=(15, 15))
plt.scatter(u[:,0], u[:,1], c=train_de_df["ServiceProcessedEnc"])
plt.title('UMAP embedding vs Service Processed Enc');

"""**Conclusion:** we do not immediately observe any distinct structure in the lower dimensional space. Using a) different representation b) different umap parameters could result in a more pronounced s

## **2. Clustering algorithm**

Links:
* [sklearn clustering](https://scikit-learn.org/stable/modules/clustering.html)

display clusters + manual labels
"""

inp = X_TFIDF
inp.shape

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

inp.shape

# setting distance_threshold=0 ensures we compute the full tree.
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
model = model.fit(inp)

# display the dendogram to select the number of clusters
fig = plt.figure(figsize=(18, 18))
plt.title('Hierarchical Clustering Dendrogram\nFor emails without associated manuals')
# plot the top three levels of the dendrogram
plot_dendrogram(model, truncate_mode='level', p=7) # p=3 'none'
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

"""**Conclusion:** the dendogram above subbest **18 clusters** might be a reasonable solution.  """

N_CLUST = 18

model2 = AgglomerativeClustering(n_clusters=N_CLUST) # distance_threshold=0, 
model2.fit(inp)

len(model2.labels_)#.predict(inp)
train_de_noman_df["labels"] = model2.labels_

# how many emails get clustered as a certain label?
train_de_noman_df["labels"].value_counts()

#from sklearn import metrics

#m1 = metrics.adjusted_mutual_info_score(true_labels, predicted_labels)  
#m2 = metrics.homogeneity_score(true_labels, predicted_labels)
#m3 = metrics.completeness_score(true_labels, predicted_labels)

#print(f"Metrics\nrand: {m1}, homogeneity: {m2}, completeness: {m3}")

#metrics.silhouette_score(train_de_df["ServiceProcessedEnc"], model2.labels_, metric='euclidean')

"""## **3. Postprocessing**

Visualize suggested clusters 

* **word cloud**
* **values of other categorical values**
* **labeling using transformers**
"""

CLUSTER_LABELS = np.unique(train_de_noman_df["labels"])

GROUP_NUM = CLUSTER_LABELS[6]         # SELECT CLUSTER GROUP TO CHARACTERIZE
sentence_group = train_de_noman_df.loc[train_de_noman_df["labels"] == GROUP_NUM]

print(f"Group {GROUP_NUM} includes {len(sentence_group)} elements")

sentence_group['body_lemma_string'] = sentence_group['body_lemma_string'].apply(lambda x: str(x).lower())
sentence_group['body_lemma_string_part'] = sentence_group['body_lemma_string_part'].apply(lambda x: str(x).lower())

from nltk import FreqDist, word_tokenize
from wordcloud import WordCloud

# FreqDist(all_words)
nltk.download('stopwords')
from nltk.corpus import stopwords
german_stop_words = stopwords.words('german')
german_stop_words.extend(["grüsse", "dank", "herr", "gut", "freundlich", 
                          "liebe", "lieber", "lg", "--", "3", "2", "1", "0", "tag",
                          "vielen", "geehrt", "dame", "mehr", "frau", "%", "kollegin", "kollege"])

all_words = " ".join(sentence_group['body_lemma_string_part']).split(" ")
all_words_filt = list(filter(lambda x: x not in german_stop_words, all_words)) 

frequency_dist = FreqDist(all_words_filt)
wordcloud = WordCloud().generate_from_frequencies(frequency_dist)

plt.figure(figsize=(16,8))
plt.imshow(wordcloud)
plt.title("The most frequent NN and NE associated with the email cluster")
plt.axis("off")
plt.show()

"""**Conclusion:** an interesting imrovement would be to explore the selection of relevant **POS** tags more"""

# Try to infer the general sentiment of sentendes => does not work
import spacy
import de_core_news_md
nlp = de_core_news_md.load()

sentence_group["sentiment"] = sentence_group['body_lemma_string'].apply(lambda x: nlp(x).sentiment )
min(sentence_group["sentiment"])

"""**Visulaize the values of other categorical variables in the cluster**"""

CATEG_VARS_SHORT = ['Impact', 'Urgency', 'IncidentType']
CATEG_VARS_LONG = ['Impact', 'Urgency', 'IncidentType', 'ServiceProcessed']

#rows, cols = (2, 3)
#fig, axs = plt.subplots(rows, cols, figsize=(15, 5))

# TODO: add other bar that corresponds to general distribution

for i, var in enumerate(CATEG_VARS_SHORT):
  fig = plt.figure(figsize=(15, 8))
  plt.title(var)
  plt.hist(sentence_group[var])
  plt.show()

# OLDER dataset exploration
train_df['ManualGroups'].dropna().value_counts()

fig = plt.figure(figsize=(8,8))
plt.hist(train_df['ManualGroups'].dropna().value_counts(), bins=20)
plt.show()



"""# **Approach 2: LDA topic modelling**

* [gensim example](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)
"""

!pip install pyLDAvis

german_stop_words = stopwords.words('german')
german_stop_words.extend(["grüsse", "dank", "herr", "gut", "freundlich", 
                          "liebe", "lieber", "lg", "--", "3", "2", "1", "0", "tag",
                          "vielen", "geehrt", "dame", "mehr", "frau", "%"])

print(f"Number of stopwords: {len(german_stop_words)}")

train_de_noman_df['body_lemma_string_vec'] = train_de_noman_df['body_lemma_string'].apply(lambda x: str(x).lower().split(" "))
train_de_noman_df['body_lemma_string_part_vec'] = train_de_noman_df['body_lemma_string_part'].apply(lambda x: str(x).lower().split(" "))

"""**9. Creating Bigram and Trigram Models**"""

data_words = train_de_noman_df['body_lemma_string_part_vec'] .copy()
#data_words

import gensim
# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=15) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=15)  

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

"""**10. Remove Stopwords, Make Bigrams and Lemmatize**"""

# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word.lower() for word in doc if word.lower() not in german_stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

#data_words_bigrams[0]

"""**11. Create the Dictionary and Corpus needed for Topic Modeling**"""

from gensim import corpora

# Create Dictionary
id2word = corpora.Dictionary(data_words_bigrams)

# Create Corpus
texts = data_words_bigrams

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

# View
print(corpus[:1])

#id2word[10]
#corpus   # very long output

# Human readable format of corpus (term-frequency)
[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]

"""**12. Building the Topic Model**"""

# select number of topics
NUM_TOPICS_GRID = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20 ]

df_lda = pd.DataFrame(columns = ["topics", "preplexity", "coherence"])
df_lda["topics"] = NUM_TOPICS_GRID
#df_lda

# Build LDA model
from pprint import pprint
from gensim.models import CoherenceModel


for num_topics in NUM_TOPICS_GRID:
  print(f"Number of topics {num_topics}")
  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=num_topics, 
                                            random_state=100,
                                            update_every=1,
                                            chunksize=100,
                                            passes=10,
                                            alpha='auto',
                                            per_word_topics=True)
  print("computing preplexity")
  df_lda.loc[df_lda["topics"] == num_topics, "preplexity"] = lda_model.log_perplexity(corpus)
  print("computing coherence")
  coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')
  df_lda.loc[df_lda["topics"] == num_topics, "coherence"] = coherence_model_lda.get_coherence()


#print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
#coherence_lda = coherence_model_lda.get_coherence()

df_lda

fig = plt.figure(figsize=(10, 8))
df_lda.plot.line(x="topics", y="coherence") # [[ "topics", "coherence"]].plot()
plt.show()

"""**Conclusion:** select the modes with the highest coherence score => either **15** or **5** topics"""

num_topics_fin = 5

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=num_topics_fin, 
                                            random_state=100,
                                            update_every=1,
                                            chunksize=100,
                                            passes=10,
                                            alpha='auto',
                                            per_word_topics=True)

"""**13. View the topics in LDA model**"""

from pprint import pprint
from gensim.models import CoherenceModel

pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

"""**15. Visualize the topics-keywordst**"""

# Visualize the topics
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
vis

"""**17. How to find the optimal number of topics for LDA?**

My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.
"""



service_df = pd.DataFrame.from_dict(service_dict)

corrMatrix = service_df.corr()

fig = plt.figure(figsize=(25, 25))
sns.heatmap(corrMatrix, annot=True, fmt='.1g')
plt.show()

"""**Display the data in lower dimensions: PCA**"""

from sklearn.decomposition import PCA
pca = PCA(n_components=2)


X_r = pca.fit_transform(service_df.transpose())
print(pca.explained_variance_ratio_)

print(pca.singular_values_)

pca_df = pd.DataFrame()
pca_df["pc1"] = X_r[:,0]
pca_df["pc2"] = X_r[:,1]
pca_df["service"] = SERVICES

from plotnine import *

ggplot(pca_df, aes(x='pc1', y='pc2', color='service'))  + geom_point() + xlab("PC1") + ylab("PC2") + ggtitle("Services")
    # scale_color_brewer(type='diverging', palette=4) +\

"""**Cluster the points**"""

#service_df.transpose()

from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=3, min_samples=2).fit(service_df.transpose())
labels = db.labels_

db.labels_

service_df.transpose()

